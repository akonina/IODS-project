# Chapter 2. Regression and model validation {.tabset}
## Data exploration
Let's start by reading the dataset that we previously wrangled.
```{r} 
learning2014 <- read.csv("./data/learning2014.csv")
``` 

For data structure, let's see
```{r} 
dim(learning2014)
str(learning2014)
head(learning2014)
``` 

These data result form a survey conducted during an Introduction to Social Statistics course in the fall of 2014. This particular dataframe contains responses from 166 people who took the exam. There are 7 data entries for each participant: gender, age, attitude towards learning, questions related to deep learning, surface learning and strategic learning, as well as the exam grade.

```{r} 
summary(learning2014$gender)
``` 
110 women and 56 men undertook the survey
```{r} 
summary(learning2014$age)
``` 
Their age ranged from 17 to 55 years old with a mean of 25.51
```{r} 
summary(learning2014$attitude)
summary(learning2014$deep)
summary(learning2014$stra)
summary(learning2014$surf)
``` 
Their attitude, as well as the questions pertaining to deep, surface and strategic learning, were measured on a scale from 1 to 5
```{r} 
summary(learning2014$points)
``` 
The exam grades varied from 7 points to 33 with a mean of 22.72
The next step is to do the graphical exploration.
```{r} 
library(GGally)
library(ggplot2)
pairs(learning2014[-1], col = learning2014$gender)
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
``` 

The plot demonstrates paired correlations between all the variables except gender. It is a binary variable and should not be simply correlated with continuous ones. Here gender is used in all of the paired plots for a more detailed visual analysis. We can also see the boxplots for each variable and analyze them visually. None of the variables contains an alarming amount of outliers except for age.

## Probability plots
Before starting any analysis, it is good to look at the probability plots of each variable.  
  
Options:  
1. ```qqnorm( )```  function to create a Quantile-Quantile plot evaluating the fit of sample data to the normal distribution.  
2. The ```fitdistr( )``` function in the MASS package provides maximum-likelihood fitting of univariate distributions. The format is ```fitdistr(x, densityfunction)``` where x is the sample data and density function is one of the following: "beta", "cauchy", "chi-squared", "exponential", "f", "gamma", "geometric", "log-normal", "lognormal", "logistic", "negative binomial", "normal", "Poisson", "t" or "weibull".  
3. For other, see: https://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
  
If the distribution does satisfy the normality criterion, log-transform the variable to identify outliers more clearly.  
Delete those observations.  
Try step 1 again.

## Boxplots for numerical variables
```
library(ggplot2)
```  
1. Create a dummy variable with the same mean and standard deviation, but normally distributed, for comparison purposes:
```
absences_norm <- rnorm(200,mean=mean(absences, na.rm=TRUE), sd=sd(absences, na.rm=TRUE))
```  
2. Plot the two variables side by side
```
boxplot(absences, absences_norm, main = "Boxplots of the absences variable: actual distribution vs. normal", at = c(1,2), names = c("absences","absences_norm"), las = 2, col = c("pink","yellow"), horizontal = TRUE)
```

## Сorrelation plots
```
library(tidyverse), library(MASS)
```  
### Option 1  
1. Calculate the correlation matrix and round it (tidyverse and corrplot packages)
```
cor_matrix <- cor(Boston) 
cor_matrix_r <- cor_matrix %>% round(2)
# округлить до 2 цифр после запятой
```
  
2. Visualize the correlation matrix
```
corrplot(cor_matrix_r, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```  
### Option 2  
1. Use ```pairs()``` command (or ggpairs with GGally package)  
2. With ```pairs()``` you can reduce the number of pairs to see the plots more clearly. After the data argument, you have to specify the columns you want to see. Ex.: ```pairs(Boston[6:10]), col = km$cluster)```

## Сenter and standardize numerical variables
```
scale()
boston_scaled <- scale(Boston), # mean = 0, центрирует вокруг нуля
class(boston_scaled) # в результате шкалирования получается матрица
boston_scaled <- as.data.frame(boston_scaled) # меняем матрицу на data frame
```

## Create a categorical variable form a numerical one
*Quantiles*  
1. Create a quantile vector of crim and print it
```
bins <- quantile(boston_scaled$crim)
bins
```  
2. Create a categorical variable 'crime'
```
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))
```  
3. Look at the table of the new factor crime
```
table(crime)
```  
4. Remove original crim from the dataset
```  
boston_scaled <- dplyr::select(boston_scaled, -crim)
```  
5. Add the new categorical value to scaled data
```  
boston_scaled <- data.frame(boston_scaled, crime)
```

## Linear regression
We are now interested in exploring whether we can explain the student exam grade by relating it with other available variables. A multiple regression model is fitted with a response variable 'points' and explanatory variables 'attitude', 'stra' and 'surf'.
```{r} 
lm <- lm(points ~ attitude + stra + surf, data = learning2014)
lm
summary(lm)
``` 

The model with three explanatory variables is able to account for 21% of the variance (multiple R squared). It shows that the only variable with a statistically significant relationship with examn grade is 'attitude'. Now we have to try out and take the two other factors one by one in a search of a more parsimonious model.
```{r} 
lm1 <- lm(points ~ attitude + stra, data = learning2014)
lm1
summary(lm1)

lm2 <- lm(points ~ attitude + surf, data = learning2014)
lm2
summary(lm2)
``` 

Taking out the 'surf' or 'stra' variables did not affect the multiple R squared and thus the fit of the model. It makes it safe to conclude that they do not affect the exam grade and we can proceed with the only significant factor 'attitude'.
```{r} 
lm3 <- lm(points ~ attitude, data = learning2014)
lm3
summary(lm3)
``` 

The fitted model explains 19% of the variance, the attitude is concluded to be a statistically significant predictor for the exam grade.

## Residual analysis
Next, to make sure that the model is fitted correctly, residuals analysis is due.

Residuals vs. Fitted values
```{r} 
plot(lm3, which = c(1))
``` 
QQ plot (theoretical quantiles)
```{r} 
plot(lm3, which = c(2))
``` 
Residuals vs. Leverage
```{r} 
plot(lm3, which = c(5))
``` 

The residuals vs. fitted values are equally distributed and do not demonsrate any pattern.
The QQ plot shows a nice fit along the line with no cause for concern.
The Leverage plot does nor show any particular outliers.  
The model is valid.

## Prediction. Train and test sets
1. Determine the number of rows in the dataset 
```
n <- nrow(boston_scaled)
```
2. Choose randomly 80% of the rows
```
ind <- sample(n,  size = n * 0.8)
```
3. Create train set
```
train <- boston_scaled[ind,]
```
4. Create test set by substraction the train set
```
test <- boston_scaled[-ind,]
```
5. Save the correct classes from test data
```
correct_classes <- test$crime (any variable used for prediction)
```
6. Remove this variable from test data
```
test <- dplyr::select(test, -crime)
```
###