# Regression and model validation {.tabset}

## Data exploration
Read the data
```{r} 
learning2014 <- read.csv("./data/learning2014.csv")
``` 

For data structure:
```{r} 
dim(learning2014)
str(learning2014)
head(learning2014)
``` 

Explore the dataset
```{r} 
summary(learning2014)
``` 

## Create a subset
```
library(dplyr)
keep <- c("Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
human <- select(human, one_of(keep))

```

## Missing values

In R, NA stands for not available, which means that the data point is missing. If a variable you wish to analyse contains missing values, there are usually two main options:  
1. Remove the observations with missing values  
2. Replace the missing values with actual values using an imputation technique.  

Option 1:  
```
# print out a completeness indicator of the 'human' data
complete.cases(human)

# print out the data along with a completeness indicator as the last column
data.frame(human[-1], comp = complete.cases(human))

# filter out all rows with NA values
human_ <- filter(human, complete.cases(human))
```
## Excluding observations
```
# look at the last 10 observations of human
tail(human, 10)

# define the last indice we want to keep
last <- nrow(human) - 7

# choose everything until the last 7 observations
human_ <- human[1:last, ]

# add countries as rownames
rownames(human_) <- human_$Country
```

## Graphical exploration

```{r} 
library(GGally)
library(ggplot2)
pairs(learning2014[-1], col = learning2014$gender)
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
``` 

## Probability plots
Before starting any analysis, it is good to look at the probability plots of each variable.  
  
Options:  
1. ```qqnorm( )```  function to create a Quantile-Quantile plot evaluating the fit of sample data to the normal distribution.  
2. The ```fitdistr( )``` function in the MASS package provides maximum-likelihood fitting of univariate distributions. The format is ```fitdistr(x, densityfunction)``` where x is the sample data and density function is one of the following: "beta", "cauchy", "chi-squared", "exponential", "f", "gamma", "geometric", "log-normal", "lognormal", "logistic", "negative binomial", "normal", "Poisson", "t" or "weibull".  
3. For other, see: https://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf
  
If the distribution does satisfy the normality criterion, log-transform the variable to identify outliers more clearly.  
Delete those observations.  
Try step 1 again.

## Boxplots for numerical variables
```
library(ggplot2)
```  
1. Create a dummy variable with the same mean and standard deviation, but normally distributed, for comparison purposes:
```
absences_norm <- rnorm(200,mean=mean(absences, na.rm=TRUE), sd=sd(absences, na.rm=TRUE))
```  
2. Plot the two variables side by side
```
boxplot(absences, absences_norm, main = "Boxplots of the absences variable: actual distribution vs. normal", at = c(1,2), names = c("absences","absences_norm"), las = 2, col = c("pink","yellow"), horizontal = TRUE)
```

## Сorrelation plots
```
library(tidyverse), library(MASS)
```  
### Option 1  
1. Calculate the correlation matrix and round it (tidyverse and corrplot packages)
```
cor_matrix <- cor(Boston) 
cor_matrix_r <- cor_matrix %>% round(2)
# округлить до 2 цифр после запятой
```
  
2. Visualize the correlation matrix
```
corrplot(cor_matrix_r, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```  
### Option 2  
1. Use ```pairs()``` command (or ggpairs with GGally package)  
2. With ```pairs()``` you can reduce the number of pairs to see the plots more clearly. After the data argument, you have to specify the columns you want to see. Ex.: ```pairs(Boston[6:10]), col = km$cluster)```

## Сenter and standardize numerical variables
```
scale()
boston_scaled <- scale(Boston), # mean = 0, центрирует вокруг нуля
class(boston_scaled) # в результате шкалирования получается матрица
boston_scaled <- as.data.frame(boston_scaled) # меняем матрицу на data frame
```

## Create a categorical variable form a numerical one
*Quantiles*  
1. Create a quantile vector of crim and print it
```
bins <- quantile(boston_scaled$crim)
bins
```  
2. Create a categorical variable 'crime'
```
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))
```  
3. Look at the table of the new factor crime
```
table(crime)
```  
4. Remove original crim from the dataset
```  
boston_scaled <- dplyr::select(boston_scaled, -crim)
```  
5. Add the new categorical value to scaled data
```  
boston_scaled <- data.frame(boston_scaled, crime)
```

## Linear regression
We are now interested in exploring whether we can explain the student exam grade by relating it with other available variables. A multiple regression model is fitted with a response variable 'points' and explanatory variables 'attitude', 'stra' and 'surf'.
```{r} 
lm <- lm(points ~ attitude + stra + surf, data = learning2014)
lm
summary(lm)
``` 

The model with three explanatory variables is able to account for 21% of the variance (multiple R squared). It shows that the only variable with a statistically significant relationship with examn grade is 'attitude'. Now we have to try out and take the two other factors one by one in a search of a more parsimonious model.
```{r} 
lm1 <- lm(points ~ attitude + stra, data = learning2014)
lm1
summary(lm1)

lm2 <- lm(points ~ attitude + surf, data = learning2014)
lm2
summary(lm2)
``` 

Taking out the 'surf' or 'stra' variables did not affect the multiple R squared and thus the fit of the model. It makes it safe to conclude that they do not affect the exam grade and we can proceed with the only significant factor 'attitude'.
```{r} 
lm3 <- lm(points ~ attitude, data = learning2014)
lm3
summary(lm3)
``` 

The fitted model explains 19% of the variance, the attitude is concluded to be a statistically significant predictor for the exam grade.

## Residual analysis
Next, to make sure that the model is fitted correctly, residuals analysis is due.

Residuals vs. Fitted values
```{r} 
plot(lm3, which = c(1))
``` 
QQ plot (theoretical quantiles)
```{r} 
plot(lm3, which = c(2))
``` 
Residuals vs. Leverage
```{r} 
plot(lm3, which = c(5))
``` 

The residuals vs. fitted values are equally distributed and do not demonsrate any pattern.
The QQ plot shows a nice fit along the line with no cause for concern.
The Leverage plot does nor show any particular outliers.  
The model is valid.

## Prediction. Train and test sets
1. Determine the number of rows in the dataset 
```
n <- nrow(boston_scaled)
```
2. Choose randomly 80% of the rows
```
ind <- sample(n,  size = n * 0.8)
```
3. Create train set
```
train <- boston_scaled[ind,]
```
4. Create test set by substraction the train set
```
test <- boston_scaled[-ind,]
```
5. Save the correct classes from test data
```
correct_classes <- test$crime (any variable used for prediction)
```
6. Remove this variable from test data
```
test <- dplyr::select(test, -crime)
```
###